---
title: "Spotify Songs"
author: "Jason Laso"
date: "12/28/2020"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

Recently there was an AI bot by The Pudding that went viral for roasting how basic people's Spotify accounts are (you can try it out here: https://pudding.cool/2020/12/judge-my-spotify/). On one hand, the bot was pretty cool and showed how powerful AI can be. On the other hand, I was personally offended by their attacks for my love of alternative rock and r&b from the early 2000s. How dare they call me old!

Look, it's not that I have anything against more modern music, but I just don't understand what type of style they are going for most of the time. I tried putting on a more recent r&b playlist (trying to impress the AI, of course) and discovered a song called "Ayo" by Chris Brown and Tyga. Chris Brown is mostly a r&b singer, Tyga is a rapper, yet "Ayo" with its catchy, upbeat chorus is clearly targeted to be a mainstream pop song. However, this only got more consfusing when I looked the song up on the Spotify API and realized they classified it as "southern hip hop". To summarize, we have a rapper and a r&b singer collaborating on a song that could be considered to be pop or rap, yet I'm first hearing it on a r&b playlist. You got all that?

This got me thinking if we could use machine learning to train a model to classify the differences between rap, r&b, and pop. Thankfully, there was a publicly available Spotify dataset from the Tidy Tuesday community to work with.


--Data Cleanup

First, we need to download data and install packages.

```{r setup}
knitr::opts_chunk$set(echo = T, cache = T, warning = F, message = F, dpi=180, fig.width = 11, fig.height = 6)
```


```{r include=FALSE}

library(tidytuesdayR)
library(ggplot2)
library(dplyr)
library(forcats)
library(tidymodels)
library(skimr)
library(fastDummies)
library(corrplot)
library(ranger)
library(kknn)
library(knitr)

# Get the Data
# Install tidytuesdayR via devtools::install_github("thebioengineer/tidytuesdayR")
tidytuesday = tidytuesdayR::tt_load('2020-01-21') 
spotify_songs = tidytuesday$spotify_songs
rm(tidytuesday)

skim(spotify_songs)
```

The release dates are very inconsistent, with several having a full "yyyy-mm-dd" format and others simply having "yyyy". For purposes of this analysis, we only want the years, which happen to be the first 4 characters for every release date string. I am also grouping them into decades (with 2020 being looped in with the 2010s since this dataset only goes through March 2020), and then removing abnormally long and short songs.

```{r}

df_spotify = spotify_songs %>% 
   filter(is.na(track_artist)==F, 
          is.na(track_name)==F) %>%
          #extracts the first 4 characters of the release date, which is the year
   mutate(year = substr(track_album_release_date, 0, 4),  
          year = as.numeric(year),
          #Groups years into decades
          decade = case_when(year <= 1979 ~ 'Pre-1980s',
                             year <= 1989 ~  '1980s',
                             year <= 1999 ~ '1990s',
                             year <= 2009 ~ '2000s',
                             year <= 2020 ~ '2010s',
                             TRUE ~ 'NA'),
          #Reorders decade as a factor to remain sequential from earliest to latest
          decade = fct_relevel(decade, levels = c('Pre-1980s', '1980s', '1990s', '2000s', '2010s')),
          #Converts milliseconds to seconds
          duration_sec = duration_ms * .001) %>%
   filter(decade != 'NA')

#This gets the 10th and 90th percentile of song lengths. Trying to remove abnormally short or long songs
duration.bounds = quantile(df_spotify$duration_sec, c(.1,.9))

df_spotify = df_spotify %>%
   filter(duration_sec >= duration.bounds[1] & 
             duration_sec <= duration.bounds[2])

df_spotify = df_spotify %>%
   select(track_id, track_artist, track_name, playlist_genre, decade) %>%
   bind_cols(df_spotify %>% select_if(is.numeric)) %>%
   select(-duration_ms)

```

The dataset has 26,264 rows, but it contains about 6,000 duplicated songs across different playlists due to artist re-releases (for example, Bon Jovi's "Livin' on a Prayer" showed up 11 different times on albums starting in 1986 and all the way through 2010). While there is no way for sure to know what the original release of each duplicated song was ("Livin' on a Prayer" had 5 distinct releases from January 1986 alone), I will assume that the most popular version of the song in the dataset is most likely to be the original release. So the chunk below will extract only the most popular release of each track and eliminate any duplicates. 

```{r}
df_spotify = df_spotify %>%
   #This sorts the data to have the most popular tracks first, which then uses the slice function to take the 1st row from each aggregate    artist/song combo
   arrange(desc(track_popularity)) %>%
   group_by(track_artist, track_name) %>%
   slice(1) %>%
   ungroup()
```

Here is a look at the breakdown of popularity by genre to look for observable trends. This chunk creates a histogram of popularity by genre.

```{r}
#Calculate the group mean popularity by genre for overlay on the plot
mean.popularity = df_spotify %>%
   group_by(playlist_genre) %>%
   summarize(mean.pop = mean(track_popularity))

ggplot(df_spotify) +
   geom_histogram(aes(track_popularity, col=track_popularity), 
                  breaks=0:100, color = 'blue', alpha = .75) +
   geom_vline(data = mean.popularity, col = 'red',
              mapping = aes(xintercept = mean.pop)) +
   geom_label(data = mean.popularity, col = 'red',
              aes(x = mean.pop, y = 300, 
                  label = paste('Mean:', round(mean.pop,1)))) +
   facet_wrap(~playlist_genre) +
   ggtitle('Song Popularity on Spotify by Genre')
          

```

The first thing that is obvious are the large amount of songs with 0 popularity, which looks consistent across genres. I don't know if this is some sort of input error or perhaps these are just far more unknown songs that don't register as many listens. To be honest, the data dictionary did not provide any info on how popularity is calculated, but I think it is fairly safe to remove them for this analysis.

```{r}
#Filters out 0 popularity songs
df_spotify = df_spotify %>%
   filter(track_popularity > 1)

#Calculate the group mean popularity by genre for overlay on the plot
mean.popularity = df_spotify %>%
   group_by(playlist_genre) %>%
   summarize(mean.pop = mean(track_popularity))

ggplot(df_spotify) +
   geom_histogram(aes(track_popularity, col=track_popularity), 
                  breaks=0:100, color = 'blue', alpha = .75) +
   geom_vline(data = mean.popularity, col = 'red',
              mapping = aes(xintercept = mean.pop)) +
   geom_label(data = mean.popularity, col = 'red',
              aes(x = mean.pop, y = 130, 
                  label = paste('Mean:', round(mean.pop,1)))) +
   facet_wrap(~playlist_genre) +
   coord_cartesian(ylim = c(0,150)) +
   ggtitle('Song Popularity on Spotify by Genre')
```
Clearly edm is the least popular genre (amen to that) with Latin and pop the most, but the important thing is seeing that other than edm the group means are relatively close to each other.

This boxplot is now looking at the popularity distribution by decade. 

```{r}

df_fewer_genres = df_spotify %>%
   filter(decade %in% c('1980s', '1990s', '2000s', '2010s'), 
          playlist_genre %in% c('rap', 'pop', 'r&b'))

#Calculate the group mean popularity by decade for overlay on the plot
mean.popularity = df_fewer_genres %>%
   group_by(decade) %>%
   summarize(mean.pop = mean(track_popularity))

ggplot(df_fewer_genres) +
   geom_boxplot(aes(decade, track_popularity, col = decade)) +
   facet_wrap(~playlist_genre) +
   #coord_cartesian(ylim = c(0,300)) +
   ggtitle('Song Popularity on Spotify by Decade & Genre')

#Removing 1980s from modeling due to lack of data points
df_fewer_genres = df_fewer_genres %>% filter(decade != '1980s')
```
Interesting to see pop music peaking in the 1980s and then dropping after that. Meanwhile, rap and r&b both saw declines in the 2000s but huge resurgences in the 2010s. It is important to note that with so much more data on 2010s songs than any other decade, it's unlikely there's much statistical significance in saying that 2010s music is more popular than any other decade (that's probably not a big surprise since I would assume that Spotify's userbase likely skews much younger). We could also see the number of outliers on 1980s rap, which is largely because of the lack of data points (rap didn't really take off on a national level until the 1990s). So I will remove all songs before 1990 from modeling. 

The dataset dcontains 12 numeric attributes about the actual sound of the music. We can use the corrplot function to create a useful heat map on the correlation between the numerical song attributes provided in the dataset.

```{r}
df_fewer_genres %>%
  select(7:17) %>% #these are the song attributes
  scale() %>%
  cor() %>%
  corrplot(method = 'color', 
                     type = 'upper', 
                     diag = F, 
                     tl.col = 'black',
                     addCoef.col = "grey30",
                     number.cex = 0.6,
                     tl.cex = .8,
                     main = 'Correlation Plot of Song Attributes',
                     mar = c(1,0,2,0))
```

Energy has a strong 68% positive correlation with Loudness, which I suppose explains why so many musicians are always asking their crowds to, "Make some noise." Energy also has a moderate negative correlation with Acousticness, which would also intuitively make some sense. To reduce collinearity, we will remove Energy from modeling.


--Model Setup

For a multi-level classification, we will try both a k-nearest neighbors and random forest model using the musical attributes. We will not include anything about time period or artist because we want to see if the computer can distinguish genres just on the sound alone. We will also use bootstrap resampling, which allows us to try modeling different splits from the original data.

```{r cache=TRUE}
set.seed(516)

#Creates a seperate data frame for just our 3 genres and decades
df_spotify_model = df_spotify %>%
   filter(decade %in% c('1990s', '2000s', '2010s') ,
          playlist_genre %in% c('rap', 'pop', 'r&b'))

#Splits the data into training and testing. Strata ensures equal distribution across genres in testing and training sets.
df_split = initial_split(df_spotify_model, 
                         strata = playlist_genre)
df_training = training(df_split)
df_testing = testing(df_split)

#Gets stratified bootstrap samples of the training data
df_bootstraps = bootstraps(df_training, 
                           strata = playlist_genre)

```

```{r, echo=FALSE, message=FALSE, include=FALSE}

#usemodels package gives boilerplate models to be used below
library(usemodels)
use_ranger(playlist_genre ~., data = df_training)

use_kknn(playlist_genre ~., data = df_training)

```

The Tidymodels package allows us to create a "recipe" for pre-processing our data before modeling. This eliminates redundancy when making multiple models. The recipe below specifies a prediction formula, eliminates non-predictor variables, removes highly correlated variables (energy), and then centers and scales the numeric attributes so that they are on level playing fields when assessing their predictive power. We then use the prep and bake functions to see what the recipe looks like when applied to the training data.

```{r, cache=T}
#Creates a recipe for consistently processing our data. Starts with writing the formula for modeling to predict genre
spotify_recipe = recipe(playlist_genre ~ ., df_training)  %>%
   #Removes all of the excess variables
   step_rm(all_nominal(),  track_popularity, year, -playlist_genre) %>%
   #Gets rid of variables with correlation over .6 (which will filter out Energy)
   step_corr(all_numeric(), threshold = .6) %>%
   #Centers and scales all numeric predictors
   step_center(all_predictors()) %>%
   step_scale(all_predictors())


#Tidymodels requires a prep step on the training data. You can then feed this into "bake", which will show the results of our recipe when applied to the training data
spotify_prepped_recipe = prep(spotify_recipe, df_training)
df_training_baked = bake(spotify_prepped_recipe, df_training)

summary(df_training_baked)

```


```{r cache=TRUE}

#Set the random forest engine
rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>% 
  set_mode("classification") %>% 
  set_engine("ranger") 

#Create the workflow to apply the model and then the pre-processing recipe
rf_workflow = workflow() %>%
   add_model(rf_model) %>%
   add_recipe(spotify_prepped_recipe)

#Apply the resampling to our processed data
rf_bootstraps = rf_workflow %>%
   tune_grid(resamples = df_bootstraps, grid = 3)

#now repeats the same 3 steps for knn model
knn_model = nearest_neighbor( ) %>%
   set_mode('classification') %>%
   set_engine('kknn')
   
knn_workflow = workflow() %>%
   add_model(knn_model) %>%
   add_recipe(spotify_prepped_recipe)

knn_bootstraps = knn_workflow %>%
   tune_grid(resamples = df_bootstraps, grid = 3)
```


```{r eval=FALSE, include=FALSE}

#I'm making a second model to predict rock, latin, and rap, which were 2 of the other genres in the original dataset. My hypothesis is that those 3 genres are totally distinct when compared to pop, rap, and r&b, and therefore the model should perform better even when holding rap constant.
#I'm not including this chunk to run again just for time purposes, but I hard-coded the results in the next chunk.

set.seed(516)

#Creates a seperate data frame for just our 3 genres and decades
df_spotify_model_rock = df_spotify %>%
   filter(decade %in% c('1990s', '2000s', '2010s') ,
          playlist_genre %in% c('rock', 'edm', 'rap'))

#Splits the data into training and testing. Strata ensures equal distribution across genres in testing and training sets.
df_split_rock = initial_split(df_spotify_model_rock, 
                         strata = playlist_genre)
df_training_rock = training(df_split_rock)
df_testing_rock = testing(df_split_rock)

#Gets stratified bootstrap samples of the training data
df_bootstraps_rock = bootstraps(df_training_rock, 
                           strata = playlist_genre)

spotify_prepped_recipe_rock = prep(spotify_recipe, df_training_rock)

#Create the workflow to apply the model and then the pre-processing recipe
rf_workflow_rock = workflow() %>%
   add_model(rf_model) %>%
   add_recipe(spotify_prepped_recipe_rock)

#Apply the resampling to our processed data
rf_bootstraps_rock = rf_workflow_rock %>%
   tune_grid(resamples = df_bootstraps_rock, grid = 1)

collect_metrics(rf_bootstraps_rock)

```

```{r include=FALSE}
#If you don't want to use the time to run a second RF for the rock model, then here were the results I got the first time, hard-coded.
rf_bootstraps_rock_metrics = data.frame(model = 'RF - rock/edm/rap', accuracy = .806, roc_auc = .929)
```

## Evaluate Models

```{r fig.height=1}

#Select best botstraps models by accuracy
rf_best = select_best(rf_bootstraps, metric = 'accuracy')
knn_best = select_best(knn_bootstraps, metric = 'accuracy')

#Gets the metrics on the training data for the best models and puts them into a table form
model_comp = collect_metrics(rf_bootstraps) %>% 
     filter(`.config` == rf_best$.config) %>% 
     mutate(model = 'rf') %>%
   bind_rows(
      collect_metrics(knn_bootstraps) %>% 
         filter(`.config` == knn_best$.config) %>% 
         mutate(model = 'knn')) %>%
   pivot_wider(names_from = .metric, values_from = mean ) %>%
   group_by(model) %>%
   summarise(accuracy = round(sum(accuracy, na.rm = T),4), 
             roc_auc = round(sum(roc_auc, na.rm = T),4))

kable(model_comp)

```

For a classification problem like this one that has fairly balanced classes, accuracy is the better metric (accuracy is a pure measure of true positives and true negatives; AUC is better for weighting false positives and negatives). The random forest model performed at 63.7% accuracy on the training data compared to just 53.1% for the knn model. I think this gap kind of makes sense given the original premise that pop, r&b, and rap have a lot of crossover in how they sound. RF will do a better job establishing specific rules for classification. If KNN misclassified one song, then by definition there's a very good chance it will also misclassify all other songs just like it (hence the name "nearest neighbors"). By comparison, a random guess would get it right 33.33%, so the RF model nearly doubling that is fairly strong. 

However, I wanted to make a benchmark for how evaluating my initial question of whether pop, rap, and r&b really are that similar. So I made a seperate random forest model for evaluating rap against 2 of the other genres in the original dataset: rock and edm. Those 3 genres are very distinct from each other in terms of sound. I would hypothesize that the accuracy of the rock model should be much higher than 63.7%, even when holding all of the rap songs constant across both models.

```{r, fig.height=1}
model_comp = collect_metrics(rf_bootstraps) %>% 
     filter(`.config` == rf_best$.config) %>% 
     mutate(model = 'RF - pop/rap/r&b') %>%
   
   pivot_wider(names_from = .metric, values_from = mean ) %>%
   group_by(model) %>%
   summarise(accuracy = round(sum(accuracy, na.rm = T),4), 
             roc_auc = round(sum(roc_auc, na.rm = T),4)) %>%
   bind_rows(bind_rows(rf_bootstraps_rock_metrics))

kable(model_comp)

```
As I suspected, the computer has a much easier time identifying the difference between rap, rock, latin as it did compared to rap, pop, and r&b. The rock/edm model was accurate on 80.6% on in-sample songs, which was nearly 17% better than the r&b/pop model!

Let's now run the RF model on the testing data and see how it does predicting genres out of sample.

```{r, cache=T}

#Selects the best metrics for the random forest by taking the most accurate bootstrap model from the training data model
final_rf = rf_workflow %>%
   finalize_workflow(select_best(rf_bootstraps, metric = 'accuracy'))

#Fits the model on the testing data
spotify_fit = last_fit(final_rf, df_split)

collect_metrics(spotify_fit)

```
Interesting that our random forest model performed slightly better on the testing (66.1% accuracy) than it did on the training data (63.7%). Typically we would expect the accuracy on the training data to be slightly higher (the model parameters would be "biased" for the training data). This likely indicates a high amount of variance in the model. In other words, there are probably a lot of songs the model classified correctly on the testing data even though it wasn't particularly confident in the choice. 

One way to look at this would be to look at the distribution of the predicted probabilities on the testing data. The RF model assigns a probability that each song belongs to rap/r&b/pop. This graph will show how confident the model was when making its selection for each song.

```{r}
model_pred = collect_predictions(spotify_fit) %>%
   bind_cols(df_testing[, c('track_artist', 'track_name')])

model_pred %>%
   mutate(.pred_max = if_else(.pred_pop > `.pred_r&b`, 
                              if_else(.pred_pop>.pred_rap, .pred_pop, .pred_rap), 
                              if_else(`.pred_r&b`>.pred_rap, `.pred_r&b`, .pred_rap)  )) %>%
   ggplot(aes(.pred_max)) +
   geom_histogram() +
   geom_vline(aes(xintercept = mean(.pred_max)), col='red') +
   geom_label(aes(x=.6, y = 80, 
                  label = paste('Avg Confidence: ', round(mean(.pred_max)*100,1 ), '%', sep='')),
              col='red') +
   xlab('Predicted Probability of Model Choice') +
   ggtitle ('Distribution of Predicted Probabilities on Testing Data')

predict(spotify_fit$.workflow[[1]], df_spotify %>% filter(track_name == 'Ayo'), type = 'prob')
```

We can see a heavy left skew in the predicted probabilities, which does confirm that the model more often than not was classifying with low confidence. It's worth reiterating that this is not a surprise given my original hypothesis. If the three genres really did sound alike, then the model would not be overly confident when making a choice. This isn't to say the results are not good (57.5% average confidence across 3 choices does show some degree of certainty), but it does offer an explanation as to why the testing data might have slightly outperformed the training data. 

How did the model do in predicting each of the 3 genre's individually? Let's check the mosaic plot.

```{r}

#Roc curve by predicted class
#model_pred %>%
 #  roc_curve(playlist_genre, .pred_pop:.pred_rap) %>%
  # autoplot()

#Data frame with confusion matrix and percentages
accuracies = model_pred %>%
   group_by(playlist_genre, .pred_class) %>%
   summarize(n = n()) %>%
   group_by(playlist_genre) %>%
   mutate(total = sum(n), pct = n/total) %>%
   mutate(x_loc = case_when(playlist_genre == 'pop' ~.1, playlist_genre == 'rap' ~ .2, TRUE ~ .3 ),
          y_loc = case_when(playlist_genre == 'pop' ~.3, playlist_genre == 'rap' ~ .2, TRUE ~ .1 ))

#Mosaic plot of confusion matrix using autoplot feature
conf_mat(model_pred, playlist_genre, .pred_class) %>%
   autoplot(type='mosaic' ) +
   ggtitle(label = 'Model Accuracy by Genre', 
           subtitle = 'R&B is Way Harder to Distinguish than Pop and Rap')
   


```
Wow! So it turns out that the model is actually very accurate distinguishing pop (77%) and rap (74%), but it is only a little bit better than random at distinguishing r&b (43%). This means that my observation that pop and rap blending together would be wrong, but I was right about what has happened to r&b. The model validates that r&b tends to sound too much like either a rap song or a pop song. 

Keep in mind, the training data was made up of nearly 90% of songs from 2010 on. As a fan of 90s and 2000s r&b, I'd be really curious if this is a trend that has evolved for more recent songs or if it was always like this and I had just never noticed. However, I do not have enough data to evaluate this trend right now.

Finally, let's see a variable importance plot to see what features were most predictive.

```{r, cache=TRUE}
library(vip)
library(pdp)

#The bootstrap models do not save the variable importance measures for each resample. It is faster to just re-run the model with the final tuning measures from earlier to save them here.
vip_rf = rf_model  %>%
   finalize_model(select_best(rf_bootstraps, metric = 'accuracy')) %>%
   set_engine('ranger', importance = 'permutation')

vip_model = workflow() %>%
   add_recipe(spotify_recipe) %>%
   add_model(vip_rf) %>%
   fit(df_training) %>%
   pull_workflow_fit()


#This makes a data frame of the VIP data 
vip.fits = vip_model$fit$variable.importance %>% 
   as.data.frame(row.names=NULL) %>% 
   bind_cols(
      vip_model$fit$variable.importance %>% 
         as.data.frame(row.names=NULL) %>% 
         row.names() %>% 
         as.data.frame() 
      ) %>%
   select(vip = `....1`, feature = `....2`) 

#VIP Plot
vip_model %>%
   vip() +
   ggtitle('Variable Importance for Predicting Rap/Pop/R&B')
   

```

The model found the most importance to be for speechiness and loudness (rap), as well as danceability and tempo (pop). Duration also showed some predictive power, ironically being a trait most associated with predicting the otherwise difficult r&b (r&b songs were about 10 seconds longer on average than rap in the training data).

